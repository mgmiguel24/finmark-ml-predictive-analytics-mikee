# finmark-ml-predictive-analytics
Homework and milestones for Machine Learning and Predictive Analytics subject
# Homework 01 — Data Preprocessing (FinMark ML Predictive Analytics)

## Task Description
This homework focuses on **data quality checking and cleaning** for three CSV datasets:
- `transactions_data.csv`
- `products_data.csv`
- `customers_data.csv`

The goal is to produce cleaned outputs that:
1) **Preserve relational integrity** (transactions must reference valid customers and products), and  
2) Separate records into **usable**, **fixable**, and **reject** buckets based on whether the row is still usable for analysis/modeling.

A key rule used:
- **Drop rows with broken relationships** (missing/invalid IDs that can’t link to lookup tables).
- Keep rows that are “fixable” (valid IDs but missing calculable fields), separated from fully clean rows.

---

## Folder Structure

### `raw_data/`
Original input datasets (unchanged):
- `transactions_data.csv` — transaction-level data (Company_ID, Product_ID, Quantity, dates, prices, costs).
- `products_data.csv` — product lookup table keyed by Product_ID.
- `customers_data.csv` — customer/company lookup table keyed by Company_ID.

### `cleaned_data/`
Final cleaned outputs generated by the notebook:

#### Transactions outputs
- `clean_transactions.csv`
  - **Usable transactions** with valid IDs (Company_ID and Product_ID exist in lookup tables) and complete required core fields.
- `fixable_transactions.csv`
  - Transactions with **valid IDs** but missing one or more “fixable” fields (e.g., Quantity / Total_Cost / Product_Price).
  - Kept separately for potential imputation or rule-based fixing.
- `reject_broken_transactions.csv`
  - Transactions with **broken relationships** (missing/invalid Company_ID and/or Product_ID).
  - Rejected to maintain relational integrity.

#### Products outputs
- `clean_products.csv`
  - Products with valid Product_ID and **referenced in clean transactions**.
- `orphan_products.csv`
  - Products with valid Product_ID but **not referenced in clean transactions**.
  - Not necessarily wrong—just unused for this dataset’s clean transaction set.
- `reject_bad_id_products.csv`
  - Products with missing Product_ID or duplicate Product_ID.
  - Rejected because Product_ID must be a stable unique key.

#### Customers outputs
- `clean_customers.csv`
  - Customers with valid Company_ID and **referenced in clean transactions**.
- `orphan_customers.csv`
  - Customers with valid Company_ID but **not referenced in clean transactions**.
- `reject_bad_id_customers.csv`
  - Customers with missing Company_ID or duplicate Company_ID.
  - Rejected because Company_ID must be a stable unique key.

---

## Notebook
- `HW1.ipynb`
  - Loads raw CSVs from `raw_data/`
  - Applies quality flags (missing/invalid/duplicate IDs, missing core fields)
  - Builds the final output tables
  - Saves outputs into `cleaned_data/`

---

## Notes / Decisions
- Relationship checks for `products` and `customers` are based on **`transactions_clean`** (not the raw transactions), so “referenced” means referenced by records that passed the relational + core field checks.
- Orphan records are kept because they may still be valid master data even if not used in the final clean transaction set.


# FINAL SANITY CHECKS (should all be 0 / True)

# 1) clean transactions should have valid IDs in clean lookups
print("Missing customer IDs in customers_clean:",
      (~transactions_clean["Company_ID"].isin(customers_clean["Company_ID"])).sum())

print("Missing product IDs in products_clean:",
      (~transactions_clean["Product_ID"].isin(products_clean["Product_ID"])).sum())

# 2) counts should add up (optional but good)
print("transactions total:", len(transactions_qc))
print("transactions buckets sum:", len(transactions_clean) + len(transactions_fixable) + len(transactions_reject_broken))

# expected must be

Missing customer IDs = 0

Missing product IDs = 0

Buckets sum = transactions total